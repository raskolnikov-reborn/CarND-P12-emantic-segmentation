{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import project_tests as ptests\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global variables for paths\n",
    "data_dir = './data'\n",
    "runs_dir = './runs'\n",
    "training_dir = data_dir + '/data_road/training'\n",
    "training_size = len(glob.glob(training_dir + '/calib/*.*'))\n",
    "vgg_dir = data_dir + '/vgg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion Successful: TF Version: 1.2.1\n"
     ]
    }
   ],
   "source": [
    "# Check Tensorflow Version\n",
    "from distutils.version import LooseVersion\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Assertion Failed. Tensorflow > 1.0 current version is {}'.format(tf.__version__)\n",
    "\n",
    "# Print version if assertion is successful\n",
    "print('Assertion Successful: TF Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU : /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "from tensorflow import test as tft\n",
    "if not tft.gpu_device_name():\n",
    "  warnings.warn('GPU not found... Please reconsider working with a GPU for training')\n",
    "else:\n",
    "  print('Default GPU : {}'.format(tft.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training constants\n",
    "num_classes_ = 2\n",
    "img_shape_ = (160, 576)\n",
    "\n",
    "# Tuned Empirically\n",
    "epochs_ = 22\n",
    "\n",
    "# Can only use 1 or max 2 with VRAM at 4 GB\n",
    "batch_size_ = 2\n",
    "\n",
    "# Learning rate is kept small because batch size is pretty minimal\n",
    "learning_rate_ = 0.0001\n",
    "\n",
    "# Dropout tuned empirically\n",
    "dropout_ = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Place_holders (_ph)\n",
    "label_ph = tf.placeholder(tf.float32, [None, \n",
    "                                       img_shape_[0],\n",
    "                                       img_shape_[1], \n",
    "                                       num_classes_])\n",
    "\n",
    "# Learning Rate\n",
    "learning_rate_ph = tf.placeholder(tf.float32)\n",
    "\n",
    "# keep_prob\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize training losses to null\n",
    "all_training_losses = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vgg(sess, vgg_dir):\n",
    "    \"\"\"\n",
    "    Load Pretrained VGG Model\n",
    "    @param sess:  Tf Session\n",
    "    @param vgg_dir: Directory containing vgg \"variables/\" and \"saved_model.pb\"\n",
    "    return: Tensors as python list\n",
    "    \"\"\"\n",
    "    # Load Model with Weights from vgg directory\n",
    "    model = tf.saved_model.loader.load(sess, ['vgg16'], vgg_dir)\n",
    "\n",
    "    # Setup tensors to get from graph ( vgg after loading)\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    # get image input\n",
    "    image_input = graph.get_tensor_by_name('image_input:0')\n",
    "\n",
    "    # get keep probability\n",
    "    keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    # Get layer outputs\n",
    "    layer_3 = graph.get_tensor_by_name('layer3_out:0')\n",
    "    layer_4 = graph.get_tensor_by_name('layer4_out:0')\n",
    "    layer_7 = graph.get_tensor_by_name('layer7_out:0')\n",
    "\n",
    "    # return as 5D list\n",
    "    return image_input, keep_prob, layer_3, layer_4, layer_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_1x1(layer, layer_name):\n",
    "  \"\"\" convolve layer by (1x1) to preserve spatial information \"\"\"\n",
    "  return tf.layers.conv2d(inputs = layer,\n",
    "                          filters =  num_classes_,\n",
    "                          kernel_size = (1, 1),\n",
    "                          strides = (1, 1),\n",
    "                          name = layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconvolve(layer, k, s, layer_name):\n",
    "  \"\"\" Transpose Convolve/ deconvolve a layer with arguments as params \"\"\"\n",
    "  return tf.layers.conv2d_transpose(inputs = layer,\n",
    "                                    filters = num_classes_,\n",
    "                                    kernel_size = (k, k),\n",
    "                                    strides = (s, s),\n",
    "                                    padding = 'same',\n",
    "                                    name = layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layers(vgg_layer_3_out,\n",
    "           vgg_layer_4_out,\n",
    "           vgg_layer_7_out,\n",
    "           num_classes = num_classes_):\n",
    "    \"\"\"\n",
    "    # Create layers for the FCN.\n",
    "    vgg_layer_n_out: TF Tensor for VGG Layer n output\n",
    "    num_classes: Number of classes to classify\n",
    "    return: The TF Tensor for the last layer of output\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Apply a 1x1 convolution to all argument layers\n",
    "    layer_3x = conv_1x1(layer = vgg_layer_3_out, layer_name = \"layer3conv1x1\")\n",
    "    layer_4x = conv_1x1(layer = vgg_layer_4_out, layer_name = \"layer4conv1x1\")\n",
    "    layer_7x = conv_1x1(layer = vgg_layer_7_out, layer_name = \"layer7conv1x1\")\n",
    "\n",
    "    # Add decoder layers to the network with skip connections\n",
    "    # Deconvolve\n",
    "    decoder_layer_1 = deconvolve(layer = layer_7x, k = 4, s = 2, layer_name = \"decoderlayer1\")\n",
    "    \n",
    "    # Sum (skip connection)\n",
    "    decoder_layer_2 = tf.add(decoder_layer_1, layer_4x, name = \"decoderlayer2\")\n",
    "    \n",
    "    # Deconvolve\n",
    "    decoder_layer_3 = deconvolve(layer = decoder_layer_2, k = 4, s = 2, layer_name = \"decoderlayer3\")\n",
    "\n",
    "    # Sum (skip connection)\n",
    "    decoder_layer_4 = tf.add(decoder_layer_3, layer_3x, name = \"decoderlayer4\")\n",
    "    \n",
    "    # Deconvolve\n",
    "    decoderlayer_output = deconvolve(layer = decoder_layer_4, k = 16, s = 8, layer_name = \"decoderlayer_output\")\n",
    "\n",
    "    return decoderlayer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layers_full(vgg_layer_3_out,\n",
    "                vgg_layer_4_out,\n",
    "                vgg_layer_7_out,\n",
    "                num_classes = num_classes_):\n",
    "\n",
    "    \"\"\"\n",
    "    # Create layers for the FCN.\n",
    "    vgg_layer_n_out: TF Tensor for VGG Layer n output\n",
    "    num_classes: Number of classes to classify\n",
    "    return: List of tensors for all layers\n",
    "    \"\"\"\n",
    "    # Apply a 1x1 convolution to encoder layers\n",
    "    layer_3x = conv_1x1(layer = vgg_layer_3_out, layer_name = \"layer3conv1x1\")\n",
    "    layer_4x = conv_1x1(layer = vgg_layer_4_out, layer_name = \"layer4conv1x1\")\n",
    "    layer_7x = conv_1x1(layer = vgg_layer_7_out, layer_name = \"layer7conv1x1\")\n",
    "\n",
    "    # Add decoder layers to the network with skip connections\n",
    "    # Deconvolve\n",
    "    decoder_layer_1 = deconvolve(layer = layer_7x, k = 4, s = 2, layer_name = \"decoderlayer1\")\n",
    "    \n",
    "    # Sum (skip connection)\n",
    "    decoder_layer_2 = tf.add(decoder_layer_1, layer_4x, name = \"decoderlayer2\")\n",
    "    \n",
    "    # Deconvolve\n",
    "    decoder_layer_3 = deconvolve(layer = decoder_layer_2, k = 4, s = 2, layer_name = \"decoderlayer3\")\n",
    "\n",
    "    # Sum (skip connection)\n",
    "    decoder_layer_4 = tf.add(decoder_layer_3, layer_3x, name = \"decoderlayer4\")\n",
    "    \n",
    "    # Deconvolve\n",
    "    decoderlayer_output = deconvolve(layer = decoder_layer_4, k = 16, s = 8, layer_name = \"decoderlayer_output\")\n",
    "\n",
    "    # Return all the layers for a more detailed output\n",
    "    return vgg_layer_3_out, vgg_layer_4_out, vgg_layer_7_out, layer_3x, layer_4x, layer_7x, \\\n",
    "         decoder_layer_1, decoder_layer_2, decoder_layer_3, decoder_layer_4, decoderlayer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes = num_classes_):\n",
    "    \"\"\"\n",
    "    TF loss and optimizer operations.\n",
    "    nn_last_layer: last layer tensor\n",
    "    correct_label: label image placeholder\n",
    "    learning_rate: learning rate placeholder\n",
    "    num_classes: Number of classes to classify\n",
    "    return: logits, train_op, cross_entropy_loss as python list\n",
    "    \"\"\"\n",
    "    # Flatten 4D tensors to 2D\n",
    "    # (pixel,class)\n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
    "    class_labels = tf.reshape(correct_label, (-1, num_classes))\n",
    "\n",
    "    # The cross_entropy_loss is the cost heuristic\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits,\n",
    "                                                            labels = class_labels)\n",
    "    # use the reduce mean method\n",
    "    cross_entropy_loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Use the standard Adam optimizer to minimize loss\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "\n",
    "    # return logits, train_op, cross_entropy_loss as python list\n",
    "    return logits, train_op, cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op,\n",
    "             cross_entropy_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate):\n",
    "    \"\"\"\n",
    "    Train the neural network and provide debug prints during training\n",
    "    Arguments: \n",
    "    sess: TF Session\n",
    "    epochs: Number of epochs\n",
    "    batch_size: Batch size\n",
    "    get_batches_fn: Function to get batches of training data\n",
    "    train_op: training operation\n",
    "    cross_entropy_loss: Loss Tensor\n",
    "    input_image: TF Placeholder for input images\n",
    "    correct_label: TF Placeholder for label images\n",
    "    keep_probkeep_prob_ph: TF Placeholder for dropout keep probability\n",
    "    learning_rate: TF Placeholder for learning rate\n",
    "    \"\"\"\n",
    "    # For all epochs\n",
    "    for epoch in range(epochs):\n",
    "        #initialize losses and counter\n",
    "        losses, i = [], 0\n",
    "        \n",
    "        # For all images in the batch\n",
    "        for images, labels in get_batches_fn(batch_size_):\n",
    "            \n",
    "            # increment batch counter by 1\n",
    "            i += 1\n",
    "            \n",
    "            # Create the feed by assigining values to placeholders\n",
    "            feed = {input_image: images,\n",
    "                    correct_label: labels,\n",
    "                    keep_prob: dropout_,\n",
    "                    learning_rate: learning_rate_ }\n",
    "\n",
    "            # Run the training op with the created feed\n",
    "            _, partial_loss = sess.run([train_op, cross_entropy_loss], feed_dict = feed)\n",
    "\n",
    "            # display output\n",
    "            print(\"- - - - - >Iteration: \", i, \"----->Partial loss:\", partial_loss)\n",
    "            \n",
    "            # Add to list of losses\n",
    "            losses.append(partial_loss)\n",
    "\n",
    "        # After each batch compute net average loss\n",
    "        training_loss = sum(losses) / len(losses)\n",
    "        \n",
    "        # Add to list of global training losses\n",
    "        all_training_losses.append(training_loss)\n",
    "\n",
    "        # Print Training loss at end of each Epoch\n",
    "        print(\"***************\")\n",
    "        print(\"Epoch: \", epoch + 1, \" of \", epochs_, \"training loss: \", training_loss)\n",
    "        print(\"***************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"\n",
    "    Run tests to test whether functions are correctly created\n",
    "    \"\"\"\n",
    "    ptests.test_layers(layers)\n",
    "    ptests.test_optimize(optimize)\n",
    "    ptests.test_for_kitti_dataset(data_dir)\n",
    "    ptests.test_train_nn(train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    \n",
    "    print(\"Training data size\", training_size)\n",
    "    \n",
    "    # download vgg model if it doesnt exist\n",
    "    helper.maybe_download_pretrained_vgg(data_dir)\n",
    "    \n",
    "    # use the get batches function from the helper.py provided\n",
    "    get_batches_fn = helper.gen_batch_function(training_dir, img_shape_)\n",
    "    \n",
    "    # Using the default session\n",
    "    with tf.Session() as session:\n",
    "        \n",
    "        # Returns the input dropout and output layers from vgg\n",
    "        image_input, keep_prob, layer_3, layer_4, layer_7 = load_vgg(session, vgg_dir)\n",
    "\n",
    "        # Create the layers and get the output\n",
    "        model_output = layers(layer_3, layer_4, layer_7, num_classes_)\n",
    "\n",
    "        # Get the logits, training op and the loss\n",
    "        logits, train_op, cross_entropy_loss = optimize(model_output, label_ph, learning_rate_ph, num_classes_)\n",
    "\n",
    "        # Initilize all variables\n",
    "        session.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "\n",
    "        # Run the training step\n",
    "        train_nn(session, epochs_, batch_size_, get_batches_fn, \n",
    "                 train_op, cross_entropy_loss, image_input,\n",
    "                 label_ph, keep_prob, learning_rate_ph)\n",
    "\n",
    "        # Save inference data\n",
    "        helper.save_inference_samples(runs_dir, data_dir, session, img_shape_, logits, keep_prob, image_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def print_network_shapes():\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Create a random 3 channel input\n",
    "        x = np.random.randn(1, 160, 576, 3)\n",
    "\n",
    "        # Create inputs, dropout and vgg out layers\n",
    "        image_input, keep_probkeep_prob_ph, layer_3, layer_4, layer_7 = load_vgg(sess, vgg_dir)\n",
    "        \n",
    "        # Create verbose layers\n",
    "        op = layers_full(layer_3, layer_4, layer_7, num_classes_)\n",
    "\n",
    "        # initialize the variables \n",
    "        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "\n",
    "        # get the layers\n",
    "        layer_3, layer_4, layer_7, layer_3x, layer_4x, layer_7x, \\\n",
    "        decoder_layer_1, skip_conn_2, decoder_layer_3, skip_conn_4, decoder_layer_5 = \\\n",
    "        sess.run(op, feed_dict = {image_input: x, keep_prob: 1.0})\n",
    "\n",
    "        print('LAYERS')\n",
    "        # Print to use in writeup\n",
    "        print(\"VGG layer 3:\\t\\t\", layer_3.shape)\n",
    "        print(\"VGG layer 4:\\t\\t\", layer_4.shape)\n",
    "        print(\"VGG layer 7:\\t\\t\", layer_7.shape)\n",
    "        print(\"Layer 3 conv1x1:\\t\", layer_3x.shape)\n",
    "        print(\"Layer 4 conv1x1:\\t\", layer_4x.shape)\n",
    "        print(\"Layer 7 conv1x1:\\t\", layer_7x.shape)\n",
    "        print(\"Deconv 1 (k=4,s=2):\\t\", decoder_layer_1.shape)\n",
    "        print(\"Skip Connection 1:\\t\", skip_conn_2.shape)\n",
    "        print(\"Deconv 2 (k=4,s=2):\\t\", decoder_layer_3.shape)\n",
    "        print(\"Skip Connection 2:\\t\", skip_conn_4.shape)\n",
    "        print(\"Deconv 3 (k=4,s=2):\\t\", decoder_layer_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the tests at once\n",
    "\n",
    "# run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size 289\n",
      "INFO:tensorflow:Restoring parameters from b'./data/vgg/variables/variables'\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 55.3993\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 48.849\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 46.6143\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 27.1361\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 27.4733\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 23.0507\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 25.2618\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 18.1179\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 17.3833\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 14.543\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 12.3896\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 13.6944\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 10.3194\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 8.48231\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 8.75918\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 7.12792\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 6.75121\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 6.89491\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 6.70044\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 5.04696\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 5.44216\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 4.99525\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 4.55564\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 4.81707\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 4.28323\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 3.47833\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 3.68741\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 3.61917\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 2.93439\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 2.52122\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 3.32572\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 2.57712\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 2.72599\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 2.36584\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 2.36449\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 2.11141\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 2.12895\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 1.8143\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 2.21595\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 2.16504\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 1.7043\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 1.91238\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 1.88694\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 2.18372\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 1.53512\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 1.60007\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 1.5216\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 1.5006\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 1.54841\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 1.48044\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 1.67216\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 1.67201\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 1.51111\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 1.6447\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 1.54616\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 1.35301\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 1.29552\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 1.37905\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 1.4562\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 1.32977\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 1.34756\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 1.3232\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 1.30449\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 1.33292\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 1.29982\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 1.24417\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 1.26317\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 1.20759\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 1.06118\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 1.12837\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 1.53381\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 1.29967\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 1.13257\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 1.14178\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 1.2256\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 1.08259\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.988999\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 1.30236\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 1.13165\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 1.06063\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 1.08438\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 1.01849\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 1.02436\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 1.26075\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 1.01352\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 1.06545\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 1.01301\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 1.05256\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 1.01238\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 1.10911\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.974845\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.932686\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 1.03936\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.987178\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.918908\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.978455\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.880347\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.94435\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.879124\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.942262\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 1.00046\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.982862\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.917649\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.931173\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.98427\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.964676\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.921441\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.871576\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.990623\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.894434\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.892889\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.846398\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.885918\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.90743\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.862261\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.920536\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.946875\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.943855\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.866939\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.961987\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.873497\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.874751\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.88598\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.838151\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.913675\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.900985\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.841454\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.892825\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.84864\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.806196\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.857428\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.859701\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.918681\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.837312\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.80549\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.882615\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.837949\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.834123\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.821157\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.842196\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.84399\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.829509\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.923423\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.811981\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.83399\n",
      "***************\n",
      "Epoch:  1  of  22 training loss:  3.93317752501\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.867195\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.788715\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.8042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  4 ----->Partial loss: 0.841451\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.815357\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.807818\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.796072\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.760904\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.755029\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.768847\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.768866\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.776447\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.787836\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.794448\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.760764\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.7689\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.756302\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.761434\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.746672\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.785267\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.769203\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.772602\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.766446\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.784659\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.777299\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.795385\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.756053\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.787979\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.78765\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.751495\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.761479\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.753587\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.756704\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.745437\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.730416\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.742321\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.73306\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.735428\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.732569\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.826902\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.768786\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.755693\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.751214\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.768314\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.746514\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.753149\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.731703\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.773328\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.721663\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.771319\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.72797\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.755476\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.763549\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.716938\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.735354\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.733333\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.760642\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.732743\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.716888\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.725294\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.729064\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.717942\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.730829\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.714609\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.729403\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.779422\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.713239\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.737056\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.710368\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.727289\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.721841\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.723978\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.740954\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.707649\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.725668\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.720019\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.743007\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.729398\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.713889\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.715305\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.755715\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.705631\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.728742\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.718586\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.70686\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.708349\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.695127\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.705076\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.701863\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.705465\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.720057\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.708025\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.726038\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.697412\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.703244\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.711035\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.693832\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.68126\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.708762\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.701567\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.697033\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.715409\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.703214\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.694917\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.681132\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.681491\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.699204\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.687906\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.712506\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.707079\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.680912\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.691683\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.712247\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.707665\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.683774\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.71008\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.700832\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.707611\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.708431\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.679452\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.70215\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.698666\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.681908\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.697665\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.668482\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.696963\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.681745\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.678072\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.68698\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.682194\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.673522\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.705737\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.704444\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.695363\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.672364\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.673668\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.671591\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.694658\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.694289\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.666463\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.674965\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.693266\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.659348\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.688571\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.678359\n",
      "***************\n",
      "Epoch:  2  of  22 training loss:  0.728873469912\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.673191\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.689357\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.680431\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.683149\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.676929\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.672122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  7 ----->Partial loss: 0.665633\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.673196\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.667799\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.666072\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.664519\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.672554\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.651822\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.657647\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.674523\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.665473\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.662675\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.663526\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.671148\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.677085\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.668002\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.671542\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.66194\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.656995\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.652626\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.683886\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.664066\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.65845\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.656975\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.658531\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.657558\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.661917\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.647371\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.65791\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.665842\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.663672\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.654636\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.659607\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.652296\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.662868\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.666906\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.643936\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.639578\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.657218\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.674472\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.643707\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.65812\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.654047\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.652011\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.681972\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.652114\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.655388\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.65207\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.640046\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.645533\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.645793\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.634081\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.644571\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.643573\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.652053\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.652756\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.644172\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.648413\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.647069\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.639922\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.654015\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.652669\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.638628\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.640992\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.636674\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.651174\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.622251\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.642139\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.65062\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.635697\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.637359\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.629407\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.629974\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.629698\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.63267\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.654576\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.623536\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.631866\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.636051\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.633063\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.645948\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.63253\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.627712\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.659382\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.650181\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.655048\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.643082\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.650863\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.632605\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.673788\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.640023\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.634239\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.63554\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.637802\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.640088\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.637053\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.63127\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.668911\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.629357\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.635373\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.635092\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.647451\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.636122\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.636014\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.661946\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.634173\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.620345\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.629513\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.634294\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.632715\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.623613\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.615546\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.607653\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.625471\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.610607\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.614708\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.628495\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.620103\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.63453\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.631047\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.617121\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.617884\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.629655\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.636721\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.608447\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.608843\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.632298\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.626341\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.606689\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.599213\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.597644\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.624318\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.598862\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.622867\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.635505\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.609312\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.604483\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.603245\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.604393\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.5976\n",
      "***************\n",
      "Epoch:  3  of  22 training loss:  0.643998604396\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.626094\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.585106\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.596304\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.60101\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.616071\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.601826\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.604131\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.609919\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.606696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  10 ----->Partial loss: 0.599106\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.607559\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.617107\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.602745\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.607376\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.599888\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.596152\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.594967\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.599387\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.609079\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.635464\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.610757\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.598423\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.638332\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.609634\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.591385\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.598339\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.616352\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.591104\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.598765\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.583885\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.604658\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.606423\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.582268\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.587264\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.584976\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.614939\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.584087\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.57506\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.590625\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.590489\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.593384\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.593108\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.584098\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.565131\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.605018\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.595895\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.567769\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.565203\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.578789\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.569594\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.581701\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.588418\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.567062\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.580665\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.57499\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.57482\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.566678\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.551899\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.564816\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.585327\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.562777\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.571078\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.566716\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.584748\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.582073\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.550745\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.577123\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.566413\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.557095\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.558555\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.549739\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.537223\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.575866\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.549967\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.586302\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.544625\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.56647\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.564894\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.544937\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.554796\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.578047\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.551564\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.544592\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.543045\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.540992\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.53745\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.528227\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.528099\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.599594\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.54042\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.552615\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.549958\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.575776\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.5663\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.533257\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.554678\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.532308\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.549891\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.568803\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.560574\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.570966\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.561293\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.57557\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.528828\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.5479\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.574359\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.547209\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.587816\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.552231\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.549066\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.552162\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.563087\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.536971\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.521372\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.52021\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.534479\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.53454\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.50491\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.514464\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.553193\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.538982\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.513035\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.526744\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.503771\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.527287\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.522637\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.512028\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.525793\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.503995\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.523216\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.535081\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.516708\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.494309\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.552312\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.494752\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.538213\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.528317\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.540043\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.484776\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.534073\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.467857\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.509397\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.491364\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.476485\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.474935\n",
      "***************\n",
      "Epoch:  4  of  22 training loss:  0.56333232522\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.528385\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.522962\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.50093\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.519451\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.453512\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.497307\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.496704\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.486601\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.46883\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.521739\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.466156\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.490871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  13 ----->Partial loss: 0.456365\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.504658\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.478952\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.466369\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.509561\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.470846\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.489576\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.49165\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.556967\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.476506\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.470671\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.483342\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.441683\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.472372\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.476596\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.456512\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.446683\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.500201\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.446431\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.427766\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.457104\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.494361\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.446553\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.467487\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.437322\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.491415\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.438004\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.394461\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.464685\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.42021\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.411727\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.459378\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.431869\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.450546\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.437982\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.462532\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.434198\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.453316\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.498053\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.458798\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.430687\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.422106\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.463194\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.426009\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.37403\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.400486\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.50139\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.410115\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.441814\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.42089\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.416689\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.397162\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.422234\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.422318\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.390233\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.495221\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.386071\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.463392\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.428355\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.433072\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.373502\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.412151\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.396137\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.388624\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.400408\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.352004\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.384745\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.375845\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.410818\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.371577\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.38674\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.413995\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.385821\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.395731\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.412097\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.379262\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.379774\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.372073\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.365637\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.366023\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.343522\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.449171\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.314316\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.318326\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.342832\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.442481\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.361102\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.326016\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.370084\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.384325\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.35601\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.351561\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.337711\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.291964\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.367581\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.334031\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.328456\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.315429\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.323949\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.36591\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.343605\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.339103\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.325833\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.283068\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.341121\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.345336\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.273857\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.328044\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.321555\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.351852\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.39325\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.328332\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.287877\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.376681\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.363111\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.275399\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.29225\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.321585\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.313792\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.274443\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.357548\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.328003\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.383195\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.359076\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.357576\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.328856\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.294556\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.452293\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.347734\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.306447\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.285334\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.271412\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.452695\n",
      "***************\n",
      "Epoch:  5  of  22 training loss:  0.405415094515\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.345822\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.334302\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.354323\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.365259\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.336101\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.30068\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.303579\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.271172\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.348388\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.298199\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.369887\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.282407\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.332557\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.25848\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.278344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  16 ----->Partial loss: 0.327434\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.385481\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.248674\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.262731\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.236278\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.415508\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.227756\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.267653\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.247097\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.415319\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.311648\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.282529\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.382887\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.27183\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.290351\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.299256\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.392488\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.268704\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.223672\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.277721\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.273752\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.306105\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.278096\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.371059\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.288341\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.255622\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.233034\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.27814\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.266613\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.34283\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.248209\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.292556\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.238142\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.252776\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.266196\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.284563\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.294599\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.296145\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.312851\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.328287\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.276222\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.208233\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.219591\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.284774\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.293683\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.238805\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.332032\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.205019\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.23361\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.339155\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.302934\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.253992\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.291568\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.276244\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.297294\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.250583\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.221713\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.23667\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.327703\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.239964\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.329374\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.278492\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.260221\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.202084\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.243358\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.210649\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.275778\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.267604\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.209024\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.34731\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.364843\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.290439\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.23933\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.319765\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.261361\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.251538\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.239417\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.282271\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.258533\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.294304\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.200477\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.207026\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.203365\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.241593\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.373707\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.259795\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.325672\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.216229\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.245905\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.269505\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.265681\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.224278\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.201331\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.202844\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.229696\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.30261\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.209466\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.28096\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.258841\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.201657\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.247943\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.249396\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.210598\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.290838\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.37988\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.241542\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.202674\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.212395\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.208944\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.220979\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.294867\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.280861\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.286913\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.276432\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.319086\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.234165\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.201784\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.254457\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.280002\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.214286\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.199816\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.194316\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.216806\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.221681\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.257609\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.165039\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.208213\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.236759\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.22155\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.188573\n",
      "***************\n",
      "Epoch:  6  of  22 training loss:  0.271841132744\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.255773\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.215158\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.217229\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.147184\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.187312\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.271185\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.281688\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.171232\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.21273\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.295597\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.263262\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.248503\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.326879\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.174613\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.294749\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.277553\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.287015\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.147849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  19 ----->Partial loss: 0.193783\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.185177\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.260905\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.307096\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.17403\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.190919\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.373845\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.17163\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.192895\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.216516\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.252623\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.227523\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.209475\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.188931\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.244866\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.208442\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.209188\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.20891\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.326427\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.227302\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.290247\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.257576\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.237855\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.237575\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.218247\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.268948\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.209893\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.163\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.198612\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.17248\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.285266\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.19619\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.207953\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.167445\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.187788\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.243933\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.219478\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.163206\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.158739\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.25375\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.218183\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.16738\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.210502\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.221107\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.192992\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.194315\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.205404\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.173645\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.276422\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.221296\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.243577\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.256055\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.20609\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.198954\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.235182\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.227217\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.201607\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.193992\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.183995\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.20987\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.203989\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.219648\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.207755\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.191337\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.238955\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.215979\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.162888\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.189707\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.198171\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.224164\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.205825\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.241837\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.183653\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.16576\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.208407\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.201622\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.222239\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.210818\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.118658\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.203914\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.148877\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.245956\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.247273\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.217295\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.160379\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.142104\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.183242\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.234483\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.118755\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.208671\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.143123\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.142164\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.130314\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.246586\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.224904\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.18139\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.271433\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.199894\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.15958\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.211903\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.180254\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.187422\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.315813\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.248052\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.186156\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.176507\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.20454\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.174795\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.200185\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.238317\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.151332\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.176097\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.231533\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.136548\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.190932\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.167629\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.168356\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.194543\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.269486\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.323462\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.223314\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.214667\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.199906\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.293768\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.164513\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.222552\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.25093\n",
      "***************\n",
      "Epoch:  7  of  22 training loss:  0.213429101531\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.22007\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.220775\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.200491\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.17404\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.188808\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.179177\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.166036\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.12872\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.14984\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.290172\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.221472\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.170715\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.183976\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.188096\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.223702\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.174622\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.221307\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.136864\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.193596\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.203334\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.185427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  22 ----->Partial loss: 0.193775\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.144648\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.167388\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.135215\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.246265\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.192191\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.143667\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.189105\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.244547\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.0987046\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.246854\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.203642\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.278448\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.184887\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.144779\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.155128\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.132957\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.178989\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.179747\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.147633\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.159546\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.151082\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.275067\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.232025\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.221035\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.143352\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.240947\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.172455\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.200617\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.153753\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.218284\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.221218\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.190612\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.159799\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.187953\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.184372\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.168069\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.182112\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.179813\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.192441\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.143696\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.131487\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.147406\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.155679\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.264325\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.190845\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.141032\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.189799\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.215414\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.180696\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.317461\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.13416\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.165396\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.13162\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.197897\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.136915\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.208849\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.133358\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.18507\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.148135\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.149479\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.182303\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.139039\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.182903\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.168468\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.176761\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.173095\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.218996\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.206787\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.190387\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.179412\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.155197\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.208576\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.126238\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.255247\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.118963\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.21447\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.139469\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.216012\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.188677\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.157542\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.195246\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.20669\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.161522\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.156761\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.183426\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.14976\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.197051\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.189678\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.20077\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.164058\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.129526\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.178329\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.14598\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.1468\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.142789\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.16574\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.139435\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.166155\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.158738\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.291253\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.188899\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.183158\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.147322\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.126836\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.227474\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.148499\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.17224\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.267252\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.209389\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.140735\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.145511\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.212732\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.193959\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.149198\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.137329\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.156589\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.161699\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.0954086\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.125904\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.140476\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.206991\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.137911\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.201028\n",
      "***************\n",
      "Epoch:  8  of  22 training loss:  0.179847412633\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.243705\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.13146\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.227097\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.137773\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.16946\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.110643\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.215609\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.156847\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.288992\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.157066\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.160921\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.200075\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.174478\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.122964\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.135563\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.146932\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.125602\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.141118\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.1904\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.119367\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.17986\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.257883\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.111978\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.256231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  25 ----->Partial loss: 0.141841\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.123607\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.202069\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.190223\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.205179\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.151827\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.159014\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.203149\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.152433\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.135781\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.132262\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.160261\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.188676\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.139658\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.122408\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.17479\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.16042\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.157628\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.147294\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.202685\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.110273\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.131707\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.0931666\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.151388\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.309561\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.205291\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.128876\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.190036\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.0989874\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.191847\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.136212\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.117049\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.156119\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.111702\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.169873\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.0712393\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.26334\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.153764\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.163341\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0953857\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.142752\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.124931\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.148973\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.141355\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.115579\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.0996491\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.142095\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.142246\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.153824\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.104128\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.193672\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.134425\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.13554\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.120336\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.211516\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.10365\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.111989\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.112098\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.175045\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.153685\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.187178\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.115943\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.152713\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.162439\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.128155\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.195486\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.137419\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.149397\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.186178\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.227628\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.151755\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.14681\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.19262\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.252644\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.188178\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.104739\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.236798\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.219058\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.188802\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.186595\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.146472\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.158272\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.234559\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.161455\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.196772\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.168258\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.114738\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.132508\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.137629\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.173966\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.167314\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.149173\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.148251\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.135594\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.172785\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.164199\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.123441\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.117143\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.0822074\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.159177\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.164326\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.167766\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.153144\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.121442\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.173399\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.107965\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.149439\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.23497\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.130049\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.134681\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.124677\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.0985803\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.139952\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.144854\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.138442\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.0862228\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.143485\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.154892\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.119551\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.202845\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.18912\n",
      "***************\n",
      "Epoch:  9  of  22 training loss:  0.157683635375\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.137667\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.136072\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.120304\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.126605\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.169228\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.163908\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.117981\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.154721\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.130914\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.146323\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.143142\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.157473\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.113863\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.113407\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.0904233\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.213707\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.139702\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.128901\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.0930371\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.13383\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.191126\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.114049\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.150474\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.177974\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.085907\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.155711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  27 ----->Partial loss: 0.151641\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.173635\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.126308\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.17721\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.152842\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.245232\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.16016\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.096618\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.211924\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.103553\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.119891\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.125301\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.155052\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.125331\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.0865884\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.127191\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.106681\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.120601\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.10989\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.0841482\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.212707\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.13552\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.148134\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.142329\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.147799\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.0925734\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.212719\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.148287\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.175919\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.0891252\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.151904\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.130894\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.12346\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.109583\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.120984\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.119817\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.122679\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0836912\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.136001\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.125647\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.175042\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.0967907\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.109381\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.0930735\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.114083\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.160013\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.167626\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.0878004\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.130835\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.140982\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.117472\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.150398\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.084469\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.160002\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.117068\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0788169\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.102707\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.121121\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.109509\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.163935\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.255069\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.158135\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.110327\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.129128\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.166295\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.198829\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.143489\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.118162\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.135826\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.105636\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.240813\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.22559\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.133652\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.0729132\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.193279\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.0864012\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.115088\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.128889\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.0846751\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.0788449\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.149662\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.113531\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.149256\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.119099\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.0775087\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.0982822\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.128499\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.084833\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.10392\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.121047\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.129712\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.112035\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.099182\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.118358\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.099381\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.166837\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.110513\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.129702\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.154556\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.13885\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.167578\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.190228\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.2314\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.133944\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.143162\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.100713\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.115265\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.132331\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.152324\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.223306\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.134024\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.213776\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.124477\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.265864\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.138059\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.118958\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.15239\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.174444\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.130995\n",
      "***************\n",
      "Epoch:  10  of  22 training loss:  0.137284032846\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.105628\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.111841\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.106641\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.154937\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.12616\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.113136\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.168177\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.122253\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.0980049\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.123747\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.109504\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.0836663\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.112412\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.0799838\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.0736969\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.117596\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.110472\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.149267\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.07795\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.114164\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.242579\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.0825054\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.218352\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.196396\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.13422\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.106158\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.135439\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.118154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  29 ----->Partial loss: 0.119505\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.114789\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.0687987\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.0690141\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.0786577\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.14915\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.119683\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.101632\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.0865654\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.0830572\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.17897\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.114081\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.181491\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.214067\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.142782\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.105522\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.172687\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.116382\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.0690083\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.126229\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.0845853\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.200208\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.0935946\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.135893\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.169713\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.109644\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.215093\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.166885\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.115012\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.111301\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.0852588\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.181073\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.0960869\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.138828\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.104243\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0950543\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.125895\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.14608\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.136143\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.162041\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.184994\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.123792\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.0901866\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.148354\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.132489\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.0999455\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.136896\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.122571\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.118911\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.0896508\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.0811818\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.13867\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.123529\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.273588\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.116722\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.104105\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.0969312\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.121684\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.144381\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.0867458\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.164497\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.110402\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.116462\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.11787\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.146452\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.120344\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.109052\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.100546\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.14573\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.173931\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.182192\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.0865792\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.102812\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.149711\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.235308\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.161416\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.0862755\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.158591\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.114377\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.138582\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.133376\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.151836\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.1298\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.118914\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.125483\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.176023\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.100824\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.114499\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.111929\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.137325\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.191667\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.1051\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.166213\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.0957117\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.165869\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.156871\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.124541\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.146716\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.139829\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.164894\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.122596\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.179138\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.170734\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.165859\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.10595\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.131328\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.212223\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.173499\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.147977\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.0729674\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.103576\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.116035\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.16263\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.118462\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.0951025\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.115113\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.149506\n",
      "***************\n",
      "Epoch:  11  of  22 training loss:  0.130581465107\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.0945243\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.0835817\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.16601\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.126049\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.164594\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.0745887\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.191223\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.0751781\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.102855\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.113762\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.114805\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.0763654\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.148722\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.0913\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.117148\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.0854927\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.0791807\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.0982234\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.106544\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.150287\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.114331\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.105925\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.149837\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.103728\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.151293\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.120967\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.182609\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.129739\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.154272\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.109205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  31 ----->Partial loss: 0.103534\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.0898415\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.103437\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.212776\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.14414\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.0920079\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.0843444\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.0755651\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.0951887\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.12542\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.135018\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.0995338\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.151501\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.172239\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.123648\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.113983\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.105068\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.0962035\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.116895\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.0768538\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.145606\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.164096\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.142587\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.085218\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.156983\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.0730764\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.105876\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.0695551\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.0987023\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.176098\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.0827329\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.177919\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.0781047\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.122879\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.102101\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.123324\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.125508\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.113782\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.0747666\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.0822357\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.0921793\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.0916145\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.142526\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.143482\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.0903999\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.115615\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.086922\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.0969292\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.0623483\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.147756\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.198127\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0842731\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.134095\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.13738\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.184376\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.0693856\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.0675456\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.112758\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.14016\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.0885157\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.108824\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.0982433\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.131577\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.0684322\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.0601671\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.0567175\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.084111\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.0847705\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.13111\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.114644\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.163112\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.101359\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.0727012\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.0918415\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.135208\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.0743101\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.170467\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.0954161\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.148452\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.0837583\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.107023\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.103533\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.0882555\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.0977433\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.146727\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.172878\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.121728\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.110613\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.139834\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.123909\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.21416\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.0768831\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.107828\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.12155\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.196594\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.0887087\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.133336\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.060142\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.185443\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.167588\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.110441\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.0805333\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.289706\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.136281\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.0983351\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.177821\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.0880779\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.100723\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.0812044\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.414637\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.106469\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.181218\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.238377\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.14223\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.132695\n",
      "***************\n",
      "Epoch:  12  of  22 training loss:  0.120727755206\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.113185\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.145613\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.106743\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.108519\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.326092\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.255803\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.195074\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.197069\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.244667\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.126999\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.214745\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.178063\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.103746\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.14187\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.160374\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.205585\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.116426\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.158225\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.228078\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.194569\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.198017\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.148082\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.11272\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.188222\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.101552\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.248818\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.103932\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.157356\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.127707\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.0812407\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.214766\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.190569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  33 ----->Partial loss: 0.124557\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.0958511\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.200446\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.1202\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.136403\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.207346\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.180602\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.118329\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.158043\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.115129\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.145495\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.172005\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.166778\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.0946253\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.130358\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.148642\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.115303\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.217225\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.119271\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.16188\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.087049\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.145232\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.158826\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.212275\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.190515\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.148752\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.131885\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.136884\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.153933\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.163814\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.24522\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.236282\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.215533\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.156496\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.113587\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.106636\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.143997\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.226982\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.244022\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.158274\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.130613\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.134549\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.176244\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.37683\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.128131\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.202736\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.0905551\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.0873911\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.11011\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.150226\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.150221\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.112868\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.0879023\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.161794\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.153614\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.0908756\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.0975582\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.137636\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.15301\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.104714\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.0861061\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.136302\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.0811004\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.132868\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.109953\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.0834208\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.116748\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.120402\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.0821957\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.16359\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.161252\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.165245\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.144238\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.0782809\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.0724631\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.0973795\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.169601\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.0767662\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.160325\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.0684555\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.107066\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.107062\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.121988\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.118238\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.211599\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.0944495\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.155632\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.120979\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.11714\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.168053\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.173339\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.271352\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.102506\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.114753\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.184901\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.0993084\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.120534\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.147965\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.216293\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.117379\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.0992897\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.107435\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.12309\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.0731299\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.136456\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.204388\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.0608772\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.11532\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.231312\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.133136\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.195198\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.142324\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.125017\n",
      "***************\n",
      "Epoch:  13  of  22 training loss:  0.148006151539\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.101281\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.155757\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.121782\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.104463\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.108985\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.136836\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.116858\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.13235\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.0874954\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.108722\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.0700578\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.13336\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.122594\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.205437\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.0847949\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.161227\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.116375\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.0946637\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.222489\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.134358\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.130155\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.0676765\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.0935762\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.106702\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.0952888\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.128606\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.180539\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.126708\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.0842503\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.0790443\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.092973\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.084103\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.134609\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.0840503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  35 ----->Partial loss: 0.136181\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.114301\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.07198\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.0927821\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.0710277\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.131172\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.0684985\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.0831961\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.103057\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.126173\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.149549\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.129171\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.104527\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.111569\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.13611\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.106821\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.111456\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.102206\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.123356\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.135379\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.0899806\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.0615517\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.0678222\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.165274\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.0748297\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.130152\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.0594221\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.10751\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.146607\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0872754\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.104908\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.161567\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.40659\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.0793914\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.142645\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.0825099\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.14351\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.137463\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.0957741\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.209512\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.106156\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.141333\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.10072\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.0753735\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.147767\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.128501\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.0811145\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0836173\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.128914\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.123015\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.0853536\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.15905\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.0997508\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.114448\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.135102\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.0999666\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.0805909\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.195205\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.165378\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.203112\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.10994\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.122013\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.0733295\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.130756\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.129985\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.14954\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.0895199\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.128049\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.132902\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.128356\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.135831\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.0889984\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.0864982\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.136119\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.135818\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.117487\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.125981\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.0600859\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.0673859\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.0908917\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.0870462\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.0673147\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.117281\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.0791923\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.103363\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.0749282\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.0533026\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.0609499\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.146784\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.24066\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.101279\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.149944\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.100443\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.0804417\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.279651\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.107015\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.134171\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.104084\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.12649\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.0879433\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.0853216\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.0798625\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.113093\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.0887853\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.0624016\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.078436\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.0780801\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.0735432\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.0879136\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.0710542\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.0772328\n",
      "***************\n",
      "Epoch:  14  of  22 training loss:  0.114916799731\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.115321\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.105994\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.0906973\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.0662427\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.0778971\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.0826553\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.132708\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.0993058\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.109365\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.0907237\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.102526\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.161511\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.0831103\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.0679486\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.0772558\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.0748981\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.12718\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.0710524\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.0729989\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.150388\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.168795\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.161451\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.0745222\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.0823274\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.0512509\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.0522084\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.107283\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.112993\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.147502\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.0946293\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.129858\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.0908762\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.108842\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.0507822\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.0739656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  36 ----->Partial loss: 0.107466\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.0707994\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.149415\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.0525071\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.114844\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.107144\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.0922919\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.0942315\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.154096\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.0747399\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.0927239\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.10284\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.195963\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.0715373\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.101498\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.079059\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.0867056\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.0799089\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.0706095\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.112654\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.141153\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.140528\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.0735677\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.0799397\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.0877364\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.102715\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.0797862\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.0958145\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0660159\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.0784737\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.149798\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.0953036\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.134608\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.113403\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.195939\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.0856\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.101608\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.160602\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.119255\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.0985812\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.175454\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.0654946\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.0771807\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.109799\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.0806787\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.062068\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0789274\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.0613524\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.121364\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.0628761\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.0474097\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.0690644\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.0726558\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.0663758\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.0979472\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.141732\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.0654997\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.0818018\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.140147\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.107276\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.0979322\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.0804902\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.0601734\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.0748747\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.154315\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.0945777\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.0986977\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.119087\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.0714443\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.0944938\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.0841079\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.0956182\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.097283\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.132913\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.0958848\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.104433\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.0700475\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.0682694\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.153862\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.115462\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.0585711\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.124053\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.0652736\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.10759\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.104071\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.0917915\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.12818\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.0840778\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.108633\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.103447\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.132334\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.118651\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.129317\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.0771977\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.133649\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.0835005\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.132638\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.051342\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.0630698\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.165678\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.0686095\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.117882\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.0856202\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.0962733\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.0713102\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.0946938\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.0880016\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.0520691\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.062119\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.0971592\n",
      "***************\n",
      "Epoch:  15  of  22 training loss:  0.098788957118\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.150382\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.112612\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.0784206\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.0638763\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.0800282\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.0462566\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.0892319\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.0617648\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.0962071\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.11386\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.14093\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.0929048\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.07612\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.0897827\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.116261\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.105098\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.109236\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.0773448\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.0585715\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.116118\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.0777795\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.0791717\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.0384568\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.0787548\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.0709737\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.113208\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.0496501\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.124412\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.105481\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.125048\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.0919011\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.168638\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.0844805\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.106606\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.0599106\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.0991662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  37 ----->Partial loss: 0.0509031\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.113644\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.103352\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.105315\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.0599523\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.138243\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.107588\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.0665015\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.0879383\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.17474\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.0827097\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.121723\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.15327\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.0804837\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.102574\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.155099\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.0831182\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.108353\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.085307\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.0721402\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.114029\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.083694\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.120127\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.116466\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.0842511\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.066476\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.083238\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0779814\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.103445\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.0887635\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.107893\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.123125\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.104035\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.063259\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.0532054\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.0839057\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.132758\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.0726137\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.0791825\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.0488866\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.114192\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.0512869\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.0845005\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.0472211\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.0904344\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0719976\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.0746963\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.079384\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.054014\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.0951506\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.0825377\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.107415\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.0880131\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.111157\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.120203\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.0995008\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.122483\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.113239\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.0661523\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.134986\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.0496881\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.101319\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.0630189\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.045041\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.104053\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.0511051\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.0707998\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.122144\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.079288\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.0695552\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.0857611\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.118602\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.0887183\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.0954157\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.104753\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.0989604\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.0565016\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.0559275\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.0769078\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.0637916\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.0888215\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.103037\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.0684497\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.0877551\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.122085\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.0531005\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.0664315\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.0425367\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.0845987\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.0999308\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.0567862\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.0675434\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.0541233\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.0722226\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.0612541\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.0822238\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.118864\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.0528625\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.0731638\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.13154\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.0883784\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.0388721\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.0693843\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.168982\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.129648\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.0930663\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.0502078\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.150514\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.170724\n",
      "***************\n",
      "Epoch:  16  of  22 training loss:  0.0907996709748\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.0832547\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.121749\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.0983188\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.0568071\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.0677895\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.083011\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.0716231\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.08389\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.105767\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.0667277\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.0904717\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.0506672\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.0686904\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.0644745\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.0648572\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.0792756\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.135696\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.119613\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.102278\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.114885\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.0951379\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.0851766\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.0783491\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.0835184\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.0583394\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.12552\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.0562446\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.0502075\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.158705\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.0509112\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.0598323\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.133645\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.0851746\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.0682765\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.100161\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.167925\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.104861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  38 ----->Partial loss: 0.0870722\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.107744\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.0974819\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.0659789\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.115382\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.0770411\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.0657091\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.0946132\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.0919337\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.0789741\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.0762558\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.0450217\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.105735\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.0434439\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.04901\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.0787233\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.0659186\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.0677489\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.0515521\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.0832772\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.0897728\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.0742855\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.107656\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.105993\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.0842016\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.0738532\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.133477\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.108688\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.0540058\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.12318\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.0520844\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.0994017\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.071323\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.0987147\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.0417201\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.0552961\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.0382475\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.0588481\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.0953921\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.0699016\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.0825124\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.163659\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.0763589\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.086499\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0590157\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.070989\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.0857631\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.0882216\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.104955\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.0870945\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.0650744\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.066985\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.0800821\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.0893502\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.174761\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.0674869\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.089561\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.0550485\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.0940575\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.0898062\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.110272\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.0628375\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.146863\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.0421962\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.0706753\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.153975\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.253555\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.0712087\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.0800567\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.0734915\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.110154\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.123476\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.0476412\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.0811364\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.0964017\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.0722427\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.105792\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.119405\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.129736\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.0884637\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.372209\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.153106\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.127817\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.156953\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.0772875\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.191713\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.144681\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.127902\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.0593653\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.11656\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.0776933\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.0727124\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.0967462\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.123493\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.0746825\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.128366\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.113635\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.125171\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.129962\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.10096\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.126312\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.130232\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.107439\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.0715087\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.102473\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.0821543\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.0947604\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.0909596\n",
      "***************\n",
      "Epoch:  17  of  22 training loss:  0.0942772836264\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.0866607\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.192862\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.114966\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.0977758\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.121364\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.0803314\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.0668827\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.10205\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.0938398\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.0717472\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.0734345\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.11067\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.0662895\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.155638\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.0782997\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.0768589\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.10849\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.0833765\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.0418064\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.0561031\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.0874501\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.0476195\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.146348\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.0945384\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.0706048\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.0816331\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.0886672\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.0846501\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.0882053\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.087414\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.0959749\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.0698739\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.0630228\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.0646248\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.125699\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.125932\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.0996839\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.067667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  39 ----->Partial loss: 0.0904309\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.12476\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.0689616\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.0505868\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.0548261\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.0684115\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.107513\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.0700272\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.101717\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.055827\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.168944\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.106149\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.0756217\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.0623027\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.0764262\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.119388\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.0647721\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.0937556\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.0474431\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.0748711\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.114084\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.0636611\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.0904739\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.109666\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.157332\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0711514\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.0943466\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.047595\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.0862551\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.0777838\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.0595775\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.0565561\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.0906019\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.0843377\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.0778424\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.0968212\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.114755\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.0672132\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.0757631\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.0515367\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.110895\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.0640574\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.057401\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0755077\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.0987224\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.0455411\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.0622731\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.074904\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.110924\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.0504347\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.0569561\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.0808348\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.0793626\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.0741612\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.0895381\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.043522\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.107079\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.0755386\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.133418\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.0528915\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.0598109\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.084044\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.0597206\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.0582707\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.101318\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.0527462\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.130054\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.0524898\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.103305\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.043513\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.082538\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.111168\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.0667443\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.111018\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.10065\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.0709767\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.0513238\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.0540763\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.0702914\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.0800893\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.0881833\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.114512\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.0385779\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.0733129\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.0556058\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.0786913\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.112941\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.0867409\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.062312\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.10597\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.0990229\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.113\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.126399\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.0823747\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.077969\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.0696463\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.0939979\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.0682436\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.0714489\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.0534997\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.0520865\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.0741949\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.062334\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.0430405\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.0770908\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.0996746\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.107692\n",
      "***************\n",
      "Epoch:  18  of  22 training loss:  0.0835573607991\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.0780183\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.103666\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.0491431\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.0994483\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.104413\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.0740496\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.126232\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.0660085\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.0560645\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.059168\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.146698\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.119034\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.0669035\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.0764211\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.0688869\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.0592815\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.10127\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.0493719\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.0473007\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.0822579\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.0419643\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.0947809\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.0800949\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.0771026\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.1258\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.0641002\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.100721\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.0525114\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.0503536\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.0842492\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.0569294\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.0835171\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.125185\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.0672062\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.0920755\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.0848097\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.0849477\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.0461647\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.0349895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  40 ----->Partial loss: 0.0946202\n",
      "- - - - - >Iteration:  41 ----->Partial loss: 0.117999\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.0723336\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.0557067\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.0646043\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.0874504\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.07884\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.0743818\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.092105\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.102632\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.0670708\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.0578088\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.0713866\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.0560604\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.0532243\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.0603401\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.0665459\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.108652\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.0620612\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.0861705\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.0545612\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.0822047\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.0793071\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.091147\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0743626\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.0651977\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.0882318\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.117597\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.0934641\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.102583\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.0509368\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.0759382\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.0460047\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.0458645\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.065101\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.0637526\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.039781\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.0534522\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.0788615\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.0929118\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.0666187\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.0650141\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0627872\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.0599663\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.0549641\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.0795382\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.0697967\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.06293\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.0768446\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.0551366\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.102068\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.0760548\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.0718816\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.0639392\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.0910834\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.042155\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.074974\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.0571089\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.116939\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.09057\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.0882594\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.0872949\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.0351219\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.0971557\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.0596016\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.122187\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.110134\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.0967874\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.0507944\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.0723362\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.0439807\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.0760703\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.0789205\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.0802724\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.0921628\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.0752376\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.109008\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.113154\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.0808881\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.10128\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.0990757\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.05344\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.0449703\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.0419561\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.0962261\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.0500822\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.07122\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.0615629\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.047518\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.0418352\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.0385982\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.160291\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.0804201\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.0920548\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.131209\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.0636007\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.135097\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.0708154\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.0503933\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.104572\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.104238\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.081934\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.183972\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.0639404\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.0999686\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.053158\n",
      "***************\n",
      "Epoch:  19  of  22 training loss:  0.0779693732488\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.102435\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.106596\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.093675\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.123396\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.146524\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.0603279\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.0595904\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.0783489\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.0673425\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.101457\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.0804133\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.0582242\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.101975\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.067239\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.0439081\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.0743389\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.0757585\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.0594813\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.0737119\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.0306887\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.173399\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.0762269\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.0660145\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.0978562\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.0953996\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.114343\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.0690686\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.107736\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.0566865\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.0813368\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.0640061\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.0378502\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.0781219\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.115889\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.051705\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.0395724\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.0728485\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.0435897\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.0833554\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.105819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  41 ----->Partial loss: 0.0460651\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.0651047\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.0714617\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.0747434\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.0731119\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.0375156\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.053118\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.0988039\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.0624344\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.0613738\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.0516361\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.0667881\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.0499377\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.0632647\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.0395932\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.0619932\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.0386859\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.0663274\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.0788597\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.0451149\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.0327279\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.0663897\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.0636199\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0735058\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.0920662\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.118797\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.12644\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.059033\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.0699226\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.115861\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.0846563\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.0804368\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.0669922\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.0556192\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.0682789\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.0598744\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.0564593\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.0550781\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.0925273\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.0801186\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.0446619\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0764154\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.117787\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.0613983\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.0637938\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.0402715\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.07752\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.0786934\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.0648987\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.0475461\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.056243\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.0728751\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.0899241\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.0904297\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.0500925\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.0754535\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.0724834\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.0381086\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.0603245\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.0661137\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.0484047\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.0724156\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.105724\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.0704698\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.0557965\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.161682\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.0940652\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.0602348\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.0915613\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.0492047\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.0981655\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.040356\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.0976112\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.101212\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.0662813\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.0758662\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.121395\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.0410727\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.0510935\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.057231\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.0608777\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.0732881\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.0565228\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.0920158\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.053453\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.0308583\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.0492831\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.040079\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.0632846\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.0505901\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.0620475\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.0573078\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.0504071\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.0433908\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.0723248\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.0379897\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.0717351\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.0425716\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.0725872\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.0664648\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.0814115\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.0447398\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.0475588\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.0435448\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.0369736\n",
      "***************\n",
      "Epoch:  20  of  22 training loss:  0.0708053618926\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.0645784\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.0758669\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.0362127\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.030798\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.0839739\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.0724063\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.0869887\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.0722487\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.0369835\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.0737483\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.0379928\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.079701\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.0455888\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.0686458\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.0613122\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.058917\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.0522737\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.0593915\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.0871546\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.0788041\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.0761733\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.150804\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.0383138\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.0843722\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.0749646\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.0619976\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.0677029\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.0678106\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.038846\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.070384\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.0351913\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.0560095\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.0608804\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.0581012\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.0645809\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.0765129\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.0578807\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.0661144\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.0417633\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.0598771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  41 ----->Partial loss: 0.0902482\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.0447927\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.03614\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.074385\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.060199\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.0416991\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.064415\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.0584352\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.0495412\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.0516629\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.0491702\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.0860724\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.0477743\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.0330117\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.0896526\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.0539957\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.0448195\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.0451999\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.0742365\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.0786946\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.0679928\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.0793551\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.0519889\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0627471\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.0436435\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.0767145\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.0467561\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.0406998\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.102481\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.0759179\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.0658769\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.0666568\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.0792028\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.0937151\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.0517094\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.0578388\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.0541123\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.046741\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.0669026\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.0900335\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.0610472\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0787687\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.0918177\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.0667918\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.0775829\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.0478481\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.106555\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.0306174\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.070705\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.0771091\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.0787363\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.0576357\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.0628854\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.118991\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.0707956\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.0791573\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.0341913\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.0620674\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.0473883\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.0613159\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.101443\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.122788\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.0652889\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.0600234\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.0598362\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.0490023\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.0866994\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.0765457\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.0563247\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.0804188\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.0678897\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.128713\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.0322889\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.0976052\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.0799575\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.0441923\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.0471357\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.0617496\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.0452338\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.0677012\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.0574095\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.0891924\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.0614435\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.050894\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.0786008\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.0652674\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.0620457\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.0761549\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.0808295\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.0907798\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.0335835\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.121383\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.0405415\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.0726748\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.0553578\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.0345693\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.101794\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.0383667\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.0707658\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.072489\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.0560164\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.0488334\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.0540539\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.0856085\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.0416381\n",
      "***************\n",
      "Epoch:  21  of  22 training loss:  0.0656568086224\n",
      "***************\n",
      "- - - - - >Iteration:  1 ----->Partial loss: 0.0319489\n",
      "- - - - - >Iteration:  2 ----->Partial loss: 0.0521568\n",
      "- - - - - >Iteration:  3 ----->Partial loss: 0.0682277\n",
      "- - - - - >Iteration:  4 ----->Partial loss: 0.0501329\n",
      "- - - - - >Iteration:  5 ----->Partial loss: 0.0326929\n",
      "- - - - - >Iteration:  6 ----->Partial loss: 0.0995378\n",
      "- - - - - >Iteration:  7 ----->Partial loss: 0.0941457\n",
      "- - - - - >Iteration:  8 ----->Partial loss: 0.0632707\n",
      "- - - - - >Iteration:  9 ----->Partial loss: 0.0706402\n",
      "- - - - - >Iteration:  10 ----->Partial loss: 0.0606503\n",
      "- - - - - >Iteration:  11 ----->Partial loss: 0.101463\n",
      "- - - - - >Iteration:  12 ----->Partial loss: 0.0816352\n",
      "- - - - - >Iteration:  13 ----->Partial loss: 0.0743367\n",
      "- - - - - >Iteration:  14 ----->Partial loss: 0.102364\n",
      "- - - - - >Iteration:  15 ----->Partial loss: 0.0850359\n",
      "- - - - - >Iteration:  16 ----->Partial loss: 0.0670802\n",
      "- - - - - >Iteration:  17 ----->Partial loss: 0.0577349\n",
      "- - - - - >Iteration:  18 ----->Partial loss: 0.0633734\n",
      "- - - - - >Iteration:  19 ----->Partial loss: 0.0764231\n",
      "- - - - - >Iteration:  20 ----->Partial loss: 0.0933721\n",
      "- - - - - >Iteration:  21 ----->Partial loss: 0.0495676\n",
      "- - - - - >Iteration:  22 ----->Partial loss: 0.0554187\n",
      "- - - - - >Iteration:  23 ----->Partial loss: 0.0578962\n",
      "- - - - - >Iteration:  24 ----->Partial loss: 0.0505757\n",
      "- - - - - >Iteration:  25 ----->Partial loss: 0.0785779\n",
      "- - - - - >Iteration:  26 ----->Partial loss: 0.0381582\n",
      "- - - - - >Iteration:  27 ----->Partial loss: 0.0521012\n",
      "- - - - - >Iteration:  28 ----->Partial loss: 0.055765\n",
      "- - - - - >Iteration:  29 ----->Partial loss: 0.0537275\n",
      "- - - - - >Iteration:  30 ----->Partial loss: 0.0431192\n",
      "- - - - - >Iteration:  31 ----->Partial loss: 0.0656372\n",
      "- - - - - >Iteration:  32 ----->Partial loss: 0.0732617\n",
      "- - - - - >Iteration:  33 ----->Partial loss: 0.0935681\n",
      "- - - - - >Iteration:  34 ----->Partial loss: 0.0636817\n",
      "- - - - - >Iteration:  35 ----->Partial loss: 0.0382776\n",
      "- - - - - >Iteration:  36 ----->Partial loss: 0.0781777\n",
      "- - - - - >Iteration:  37 ----->Partial loss: 0.0434582\n",
      "- - - - - >Iteration:  38 ----->Partial loss: 0.0770228\n",
      "- - - - - >Iteration:  39 ----->Partial loss: 0.0581152\n",
      "- - - - - >Iteration:  40 ----->Partial loss: 0.0405527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - >Iteration:  41 ----->Partial loss: 0.114239\n",
      "- - - - - >Iteration:  42 ----->Partial loss: 0.0821182\n",
      "- - - - - >Iteration:  43 ----->Partial loss: 0.0581902\n",
      "- - - - - >Iteration:  44 ----->Partial loss: 0.0556173\n",
      "- - - - - >Iteration:  45 ----->Partial loss: 0.0584006\n",
      "- - - - - >Iteration:  46 ----->Partial loss: 0.0408324\n",
      "- - - - - >Iteration:  47 ----->Partial loss: 0.0327828\n",
      "- - - - - >Iteration:  48 ----->Partial loss: 0.0478516\n",
      "- - - - - >Iteration:  49 ----->Partial loss: 0.0502843\n",
      "- - - - - >Iteration:  50 ----->Partial loss: 0.0715486\n",
      "- - - - - >Iteration:  51 ----->Partial loss: 0.0777873\n",
      "- - - - - >Iteration:  52 ----->Partial loss: 0.0598442\n",
      "- - - - - >Iteration:  53 ----->Partial loss: 0.136695\n",
      "- - - - - >Iteration:  54 ----->Partial loss: 0.0721554\n",
      "- - - - - >Iteration:  55 ----->Partial loss: 0.039593\n",
      "- - - - - >Iteration:  56 ----->Partial loss: 0.0635408\n",
      "- - - - - >Iteration:  57 ----->Partial loss: 0.0517153\n",
      "- - - - - >Iteration:  58 ----->Partial loss: 0.0620862\n",
      "- - - - - >Iteration:  59 ----->Partial loss: 0.0606808\n",
      "- - - - - >Iteration:  60 ----->Partial loss: 0.0752819\n",
      "- - - - - >Iteration:  61 ----->Partial loss: 0.0586936\n",
      "- - - - - >Iteration:  62 ----->Partial loss: 0.0901028\n",
      "- - - - - >Iteration:  63 ----->Partial loss: 0.080868\n",
      "- - - - - >Iteration:  64 ----->Partial loss: 0.0792187\n",
      "- - - - - >Iteration:  65 ----->Partial loss: 0.0488224\n",
      "- - - - - >Iteration:  66 ----->Partial loss: 0.0710803\n",
      "- - - - - >Iteration:  67 ----->Partial loss: 0.0370864\n",
      "- - - - - >Iteration:  68 ----->Partial loss: 0.0955677\n",
      "- - - - - >Iteration:  69 ----->Partial loss: 0.0804989\n",
      "- - - - - >Iteration:  70 ----->Partial loss: 0.0441605\n",
      "- - - - - >Iteration:  71 ----->Partial loss: 0.0508679\n",
      "- - - - - >Iteration:  72 ----->Partial loss: 0.0700386\n",
      "- - - - - >Iteration:  73 ----->Partial loss: 0.0694864\n",
      "- - - - - >Iteration:  74 ----->Partial loss: 0.060152\n",
      "- - - - - >Iteration:  75 ----->Partial loss: 0.0522148\n",
      "- - - - - >Iteration:  76 ----->Partial loss: 0.0648213\n",
      "- - - - - >Iteration:  77 ----->Partial loss: 0.0677946\n",
      "- - - - - >Iteration:  78 ----->Partial loss: 0.0405644\n",
      "- - - - - >Iteration:  79 ----->Partial loss: 0.0573716\n",
      "- - - - - >Iteration:  80 ----->Partial loss: 0.0377656\n",
      "- - - - - >Iteration:  81 ----->Partial loss: 0.0328526\n",
      "- - - - - >Iteration:  82 ----->Partial loss: 0.0560377\n",
      "- - - - - >Iteration:  83 ----->Partial loss: 0.0679865\n",
      "- - - - - >Iteration:  84 ----->Partial loss: 0.0612065\n",
      "- - - - - >Iteration:  85 ----->Partial loss: 0.0978565\n",
      "- - - - - >Iteration:  86 ----->Partial loss: 0.0408533\n",
      "- - - - - >Iteration:  87 ----->Partial loss: 0.0730968\n",
      "- - - - - >Iteration:  88 ----->Partial loss: 0.0821289\n",
      "- - - - - >Iteration:  89 ----->Partial loss: 0.0794079\n",
      "- - - - - >Iteration:  90 ----->Partial loss: 0.0765126\n",
      "- - - - - >Iteration:  91 ----->Partial loss: 0.0596005\n",
      "- - - - - >Iteration:  92 ----->Partial loss: 0.0538376\n",
      "- - - - - >Iteration:  93 ----->Partial loss: 0.0561927\n",
      "- - - - - >Iteration:  94 ----->Partial loss: 0.0476307\n",
      "- - - - - >Iteration:  95 ----->Partial loss: 0.0474349\n",
      "- - - - - >Iteration:  96 ----->Partial loss: 0.070035\n",
      "- - - - - >Iteration:  97 ----->Partial loss: 0.0435619\n",
      "- - - - - >Iteration:  98 ----->Partial loss: 0.0364684\n",
      "- - - - - >Iteration:  99 ----->Partial loss: 0.0828325\n",
      "- - - - - >Iteration:  100 ----->Partial loss: 0.0649628\n",
      "- - - - - >Iteration:  101 ----->Partial loss: 0.0986612\n",
      "- - - - - >Iteration:  102 ----->Partial loss: 0.13315\n",
      "- - - - - >Iteration:  103 ----->Partial loss: 0.0513144\n",
      "- - - - - >Iteration:  104 ----->Partial loss: 0.0355018\n",
      "- - - - - >Iteration:  105 ----->Partial loss: 0.071001\n",
      "- - - - - >Iteration:  106 ----->Partial loss: 0.0449597\n",
      "- - - - - >Iteration:  107 ----->Partial loss: 0.0544321\n",
      "- - - - - >Iteration:  108 ----->Partial loss: 0.0995121\n",
      "- - - - - >Iteration:  109 ----->Partial loss: 0.0388508\n",
      "- - - - - >Iteration:  110 ----->Partial loss: 0.0774466\n",
      "- - - - - >Iteration:  111 ----->Partial loss: 0.0808863\n",
      "- - - - - >Iteration:  112 ----->Partial loss: 0.0412963\n",
      "- - - - - >Iteration:  113 ----->Partial loss: 0.0508801\n",
      "- - - - - >Iteration:  114 ----->Partial loss: 0.067942\n",
      "- - - - - >Iteration:  115 ----->Partial loss: 0.0636741\n",
      "- - - - - >Iteration:  116 ----->Partial loss: 0.0783064\n",
      "- - - - - >Iteration:  117 ----->Partial loss: 0.0694235\n",
      "- - - - - >Iteration:  118 ----->Partial loss: 0.0663983\n",
      "- - - - - >Iteration:  119 ----->Partial loss: 0.130712\n",
      "- - - - - >Iteration:  120 ----->Partial loss: 0.0974343\n",
      "- - - - - >Iteration:  121 ----->Partial loss: 0.0770996\n",
      "- - - - - >Iteration:  122 ----->Partial loss: 0.0841188\n",
      "- - - - - >Iteration:  123 ----->Partial loss: 0.0838553\n",
      "- - - - - >Iteration:  124 ----->Partial loss: 0.0696064\n",
      "- - - - - >Iteration:  125 ----->Partial loss: 0.0982968\n",
      "- - - - - >Iteration:  126 ----->Partial loss: 0.0716052\n",
      "- - - - - >Iteration:  127 ----->Partial loss: 0.0702807\n",
      "- - - - - >Iteration:  128 ----->Partial loss: 0.053527\n",
      "- - - - - >Iteration:  129 ----->Partial loss: 0.0962502\n",
      "- - - - - >Iteration:  130 ----->Partial loss: 0.0674913\n",
      "- - - - - >Iteration:  131 ----->Partial loss: 0.060697\n",
      "- - - - - >Iteration:  132 ----->Partial loss: 0.0363554\n",
      "- - - - - >Iteration:  133 ----->Partial loss: 0.0860992\n",
      "- - - - - >Iteration:  134 ----->Partial loss: 0.0617057\n",
      "- - - - - >Iteration:  135 ----->Partial loss: 0.0374563\n",
      "- - - - - >Iteration:  136 ----->Partial loss: 0.0757568\n",
      "- - - - - >Iteration:  137 ----->Partial loss: 0.0432841\n",
      "- - - - - >Iteration:  138 ----->Partial loss: 0.0743602\n",
      "- - - - - >Iteration:  139 ----->Partial loss: 0.0448537\n",
      "- - - - - >Iteration:  140 ----->Partial loss: 0.073675\n",
      "- - - - - >Iteration:  141 ----->Partial loss: 0.0780705\n",
      "- - - - - >Iteration:  142 ----->Partial loss: 0.111227\n",
      "- - - - - >Iteration:  143 ----->Partial loss: 0.0470546\n",
      "- - - - - >Iteration:  144 ----->Partial loss: 0.0859835\n",
      "- - - - - >Iteration:  145 ----->Partial loss: 0.0304348\n",
      "***************\n",
      "Epoch:  22  of  22 training loss:  0.0659498450571\n",
      "***************\n",
      "Training Finished. Saving test images to: ./runs/1504346224.7774749\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-157c9bda2cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-26c4d1e0d722>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Save inference data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_inference_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_shape_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/SDC/CarND-P12-emantic-segmentation/helper.py\u001b[0m in \u001b[0;36msave_inference_samples\u001b[0;34m(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\u001b[0m\n\u001b[1;32m    137\u001b[0m     image_outputs = gen_test_output(\n\u001b[1;32m    138\u001b[0m         sess, logits, keep_prob, input_image, os.path.join(data_dir, 'data_road/testing'), image_shape)\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/SDC/CarND-P12-emantic-segmentation/helper.py\u001b[0m in \u001b[0;36mgen_test_output\u001b[0;34m(sess, logits, keep_prob, image_pl, data_folder, image_shape)\u001b[0m\n\u001b[1;32m    115\u001b[0m         im_softmax = sess.run(\n\u001b[1;32m    116\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             {keep_prob: 1.0, image_pl: [image]})\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mim_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_softmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0msegmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mim_softmax\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_network_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.9331775250106022, 0.7288734699117726, 0.64399860439629386, 0.56333232522010801, 0.40541509451537294, 0.27184113274360527, 0.21342910153084788, 0.1798474126334848, 0.15768363537459537, 0.13728403284631926, 0.13058146510658594, 0.12072775520641228, 0.14800615153949837, 0.11491679973129569, 0.098788957117960374, 0.090799670974756108, 0.094277283626383754, 0.083557360799148162, 0.077969373248774426, 0.070805361892642651, 0.065656808622438334, 0.06594984505710931]\n"
     ]
    }
   ],
   "source": [
    "print(all_training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc4ef00c588>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHLdJREFUeJzt3XuQXPWZ3vHv29e59EiDplsgdLEAawFjMJCxjLHXlu14\nC7BjkqyTQO2aDZuyDMum7MS7qV2nyt5NVaq2srveNeCgko0LU/HacWKWVbkgDsaAYbNcBhnERYAl\nYK0RoGmNNNLce7r7zR/nzGg0mktL0zM9fc7zqerqc04fdb/qmnr69Nu/8zvm7oiISLQkGl2AiIjU\nn8JdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRFCqUS+cz+d98+bNjXp5\nEZGm9Oyzzx5298JC+zUs3Ddv3kxPT0+jXl5EpCmZ2T/Wsp/aMiIiEVRzuJtZ0sx+YWY/nuUxM7Pb\nzWyfme0xsyvrW6aIiJyO0zly/yKwd47HrgW2hLftwF2LrEtERBahpnA3sw3Ap4Bvz7HL9cC9HngS\n6DSzdXWqUURETlOtR+5/DfwnoDrH4+uBA9PWe8NtIiLSAAuGu5l9Guhz92cX+2Jmtt3Mesysp1gs\nLvbpRERkDrUcuX8I+IyZvQn8APi4mf2PGfscBDZOW98QbjuJu+9092537y4UFhymKSIiZ2jBcHf3\nP3b3De6+GbgB+Jm7//aM3XYBN4WjZq4Cjrn72/UvF159Z5A//8krDIyUluLpRUQi4YzHuZvZLWZ2\nS7j6APA6sA/4FvB7dahtVm8cHuabj+yn9+joUr2EiEjTO60zVN39UeDRcHnHtO0O3FbPwuZS6MgA\ncHhofDleTkSkKTXdGar5XBaA/iG1ZURE5tJ04d4VhruO3EVE5tZ04d6eSdKSTijcRUTm0XThbmbk\nc1m1ZURE5tF04Q5Ba6aoI3cRkTk1ZbgXchkO68hdRGROTRnuQVtGR+4iInNpynDvymXoHy5RrXqj\nSxERWZGaMtzzuSyVqjMwOtHoUkREVqSmDXdArRkRkTk0Zbh35YIpCDRiRkRkdk0Z7oWps1Q1YkZE\nZDZNGe5qy4iIzK8pw311a5pkwjQFgYjIHJoy3BMJo6s9w+FBtWVERGbTlOEO4YlMwzpyFxGZTdOG\ne1cuQ1E/qIqIzGrBcDezFjN72syeN7OXzOxPZ9lnm5kdM7PnwttXl6bcEwq5LIcHdeQuIjKbWi6z\nNw583N2HzCwNPGFmD7r7kzP2e9zdP13/EmcXTEEwjrtjZsv1siIiTWHBI3cPDIWr6fDW8Eld8rks\nYxNVhkuVRpciIrLi1NRzN7OkmT0H9AEPuftTs+x2tZntMbMHzeySOZ5nu5n1mFlPsVhcRNknxrqr\nNSMicqqawt3dK+5+ObAB2Gpm752xy25gk7tfBtwB3D/H8+x092537y4UCoupe2oKAo2YERE51WmN\nlnH3AeAR4JoZ249Ptm7c/QEgbWb5ulU5i8kj96LGuouInKKW0TIFM+sMl1uBTwKvzNjnHAt/1TSz\nreHz9te/3BMKHeEUBDpyFxE5RS2jZdYB3zWzJEFo/9Ddf2xmtwC4+w7gs8CtZlYGRoEb3H1Jf3Rd\n0x60ZXSWqojIqRYMd3ffA1wxy/Yd05bvBO6sb2nzSycTdLalNb+MiMgsmvYMVdAUBCIic2nqcNfk\nYSIis2vqcM93ZNWWERGZRVOHeyGncBcRmU1Th3tXe4bjY2XGy5qCQERkuqYO9/zkWHdN/SsicpLm\nDvecwl1EZDZNHe6T88uo7y4icrKmDvfC5PwyCncRkZM0dbhPzQyptoyIyEmaOtzbMinaMkm1ZURE\nZmjqcIfgR1WFu4jIySIQ7hm1ZUREZmj6cO/SkbuIyCmaPtzVlhEROVXTh3shl+HIcIlKdUmvDSIi\n0lRqucxei5k9bWbPm9lLZvans+xjZna7me0zsz1mduXSlHuqrlyWqsPREfXdRUQm1XLkPg583N3f\nB1wOXGNmV83Y51pgS3jbDtxV1yrnMTkFgVozIiInLBjuHhgKV9PhbWYP5Hrg3nDfJ4FOM1tX31Jn\npxOZREROVVPP3cySZvYc0Ac85O5PzdhlPXBg2npvuG3J6chdRORUNYW7u1fc/XJgA7DVzN57Ji9m\nZtvNrMfMeorF4pk8xSmm5pcZVLiLiEw6rdEy7j4APAJcM+Ohg8DGaesbwm0z//1Od+929+5CoXC6\ntc5qVWuKdNLoH1ZbRkRkUi2jZQpm1hkutwKfBF6Zsdsu4KZw1MxVwDF3f7vu1c5eH13tWQ7ryF1E\nZEqqhn3WAd81syTBh8EP3f3HZnYLgLvvAB4ArgP2ASPAzUtU76zyHRn13EVEplkw3N19D3DFLNt3\nTFt24Lb6lla7rvas2jIiItM0/RmqEE5BoLaMiMiUaIR7R4bDQyWCLxAiIhKNcG/PUqpUGRwvN7oU\nEZEVIRrh3hFeKFutGRERICrhHp7IpB9VRUQCkQj3rvZwCgIduYuIABEJ96m2jMa6i4gAEQn3NW0Z\nzOCwZoYUEQEiEu6pZIKz2nSWqojIpEiEO0A+p3AXEZkUoXDP6oIdIiKhyIR7Vy6rI3cRkVBkwj1o\ny+jIXUQEIhXuWYbGy4xNVBpdiohIw0Uo3DXWXURkUoTCffJC2WrNiIhEJty7JueX0ZG7iEhN11Dd\naGaPmNnLZvaSmX1xln22mdkxM3suvH11acqdm9oyIiIn1HIN1TLwZXffbWYdwLNm9pC7vzxjv8fd\n/dP1L7E2asuIiJyw4JG7u7/t7rvD5UFgL7B+qQs7XS3pJLlsSkfuIiKcZs/dzDYTXCz7qVkevtrM\n9pjZg2Z2yRz/fruZ9ZhZT7FYPO1iF6Kx7iIigZrD3cxywI+AL7n78RkP7wY2uftlwB3A/bM9h7vv\ndPdud+8uFApnWvOcdKFsEZFATeFuZmmCYP+eu98383F3P+7uQ+HyA0DazPJ1rbQGXbkM/cMKdxGR\nWkbLGHA3sNfdvz7HPueE+2FmW8Pn7a9nobXI57Jqy4iIUNtomQ8BnwNeMLPnwm1fATYBuPsO4LPA\nrWZWBkaBG9zdl6DeeeVzWY6OlChXqqSSkRnCLyJy2hYMd3d/ArAF9rkTuLNeRZ2pfC6DOxwZKbG2\no6XR5YiINEykDm+nxroPqjUjIvEWrXDvmDyRST+qiki8RSrcu9qDKQg0YkZE4i5S4T515K62jIjE\nXKTCvSObIpNKqC0jIrEXqXA3M/LtmoJARCRS4Q5Ba0ZH7iISd9EL95zCXUQkcuHe1Z6hX20ZEYm5\nyIV7viNL//A4DZj9QERkxYheuOeyTFSc46PlRpciItIwEQz34ESmovruIhJjEQx3TUEgIhLZcNeP\nqiISZ5EL966wLaMjdxGJs8iF+1ltGRKmcBeReKvlMnsbzewRM3vZzF4ysy/Oso+Z2e1mts/M9pjZ\nlUtT7sKSCWONpiAQkZir5TJ7ZeDL7r7bzDqAZ83sIXd/edo+1wJbwtsHgLvC+4bQWaoiEncLHrm7\n+9vuvjtcHgT2Autn7HY9cK8HngQ6zWxd3autkcJdROLutHruZrYZuAJ4asZD64ED09Z7OfUDYNl0\n5TQFgYjEW83hbmY54EfAl9z9+Jm8mJltN7MeM+spFotn8hQ10ZG7iMRdTeFuZmmCYP+eu983yy4H\ngY3T1jeE207i7jvdvdvduwuFwpnUW5N8LstIqcJISVMQiEg81TJaxoC7gb3u/vU5dtsF3BSOmrkK\nOObub9exztMyOdZdrRkRiataRst8CPgc8IKZPRdu+wqwCcDddwAPANcB+4AR4Ob6l1q7QniWanFo\nnI1r2hpZiohIQywY7u7+BGAL7OPAbfUqarGm5pcZVN9dROIpcmeowrS2zLDaMiIST5EOdx25i0hc\nRTLcs6kkq1pSGg4pIrEVyXCHcKy72jIiElPRDne1ZUQkpqIb7h0ZtWVEJLYiG+5d7VmNlhGR2Ips\nuOdzWQZGJpioVBtdiojIsotuuHdoCgIRia/IhntXe3iWqvruIhJDkQ33QoculC0i8RXZcJ+aX0Zt\nGRGJociGe1cY7v06cheRGIpsuLdnkrSkE2rLiEgsRTbczSy83J7aMiISP5ENdwhaMzpyF5E4inS4\nF3IZHbmLSCzVcg3V75hZn5m9OMfj28zsmJk9F96+Wv8yz0xXe1Y/qIpILNVyDdV7gDuBe+fZ53F3\n/3RdKqqjfEeG/uES1aqTSMx7pUARkUhZ8Mjd3X8OHFmGWuoun8tSqToDoxONLkVEZFnVq+d+tZnt\nMbMHzeySOj3nommsu4jEVT3CfTewyd0vA+4A7p9rRzPbbmY9ZtZTLBbr8NLzy4fXUi0q3EUkZhYd\n7u5+3N2HwuUHgLSZ5efYd6e7d7t7d6FQWOxLL6igKQhEJKYWHe5mdo6ZWbi8NXzO/sU+bz2oLSMi\ncbXgaBkz+z6wDcibWS/wNSAN4O47gM8Ct5pZGRgFbnB3X7KKT0Nna5pkwnQik4jEzoLh7u43LvD4\nnQRDJVecRMLoas9weFBtGRGJl0ifoQpBa6Z/WEfuIhIvkQ/3fC5DUT+oikjMRD7cC7kshwd15C4i\n8RL5cO/KZegfHmeF/MYrIrIsIh/u+VyWsYkqw6VKo0sREVk2sQh3QK0ZEYmVyId7VzgFgUbMiEic\nRD7cJ4/cixrrLiIxEvlwL3RMzi+jI3cRiY/Ih/ua9rAto7HuIhIjkQ/3dDJBZ1taR+4iEiuRD3cI\n+u4KdxGJk1iEe1d7Rm0ZEYmVWIR7vkNH7iISL/EI9/aMLrUnIrESj3DPZRkcKzNe1hQEIhIP8Qj3\njsnL7anvLiLxsGC4m9l3zKzPzF6c43Ezs9vNbJ+Z7TGzK+tf5uJ0hWPd1XcXkbio5cj9HuCaeR6/\nFtgS3rYDdy2+rPrSkbuIxM2C4e7uPweOzLPL9cC9HngS6DSzdfUqsB4Kk/PL6MhdRGKiHj339cCB\naeu94bZTmNl2M+sxs55isViHl67N5MyQasuISFws6w+q7r7T3bvdvbtQKCzb67ZlUrRlkmrLiEhs\n1CPcDwIbp61vCLetKJqCQETipB7hvgu4KRw1cxVwzN3frsPz1lVXLqNwF5HYSC20g5l9H9gG5M2s\nF/gakAZw9x3AA8B1wD5gBLh5qYpdjHwuy4EjI40uQ0RkWSwY7u5+4wKPO3Bb3SpaIvlcll/86mij\nyxARWRaxOEMVIJ/LcGS4RKXqjS5FRGTJxSjcs1Qdjo5oxIyIRF+swh001l1E4iE24T55IpPGuotI\nHMQm3HXkLiJxEptwn5pfZlDhLiLRF5twX9WaIp00+ofVlhGR6ItNuJsZXe1ZnvjlYX72yiHGJnRV\nJhGJrgVPYoqSf9W9gbufeIPfvaeHlnSCD7+7wCcuXssnLlrL2lUtjS5PRKRuLDjBdPl1d3d7T0/P\nsr/u2ESFp944wsN7D/Hw3j4ODowCcOn61Xzi4rX804vP5pJzV2Fmy16biMhCzOxZd+9ecL+4hft0\n7s6rhwZ5eG8fD+89xC8ODOAOZ6/K8vGLzuYTF63lQ+/O05pJNrROEZFJCvczcHhonEde6eNnr/Tx\n89eKDJcqZFMJPvzuPB+/eC3bLlzL+s7WRpcpIjGmcF+k8XKFp984wsN7+/jp3kP0Hg3aN1vW5th2\nYYFtF66le/NZZFM6qheR5aNwryN3Z1/fEI++WuTR1/p45o2jlCpV2jJJrr4gz7YLC3z01wpsXNPW\n6FJFJOJqDfdYjZY5U2bGlrM72HJ2B5//yPkMj5f5h/39PPpaH4++WuSnew8BcEGhnW0XrmXbhQW2\nnrdGR/Ui0jA6cl8kd2d/cZjHXivy6Kt9PPXGEUrlKq3pJB+8oIttFxa49r3rKHRkG12qiERAXdsy\nZnYN8A0gCXzb3f9sxuPbgL8D3gg33efu/2W+54xKuM80Uirz5Ov9QQvn1SK/OjJCWybJLR+9gM//\n+vkaeSMii1K3cDezJPAa8EmgF3gGuNHdX562zzbgD9z907UWGNVwn+m1Q4P81UOv8eCL73D2qix/\n8BsX8i+v3EAyoXH0InL6ag33WqYf2Arsc/fX3b0E/AC4frEFxsWvnd3BXb/9T/hft3yQc1a38of/\new//7I4n+Pt9hxtdmohEWC3hvh44MG29N9w209VmtsfMHjSzS+pSXYS8f/Ma7v+9q7n9xis4NjrB\nb337KX73nmf45aHBRpcmIhFUr4nDdgOb3P0y4A7g/tl2MrPtZtZjZj3FYrFOL908zIzPvO9cHv7y\nR/njay/imTePcM03Huc//+0LmopYROqqlnA/CGyctr4h3DbF3Y+7+1C4/ACQNrP8zCdy953u3u3u\n3YVCYRFlN7eWdJIvfPQCHvvDj/G5q97F/3zmAB/7i0f55iP7NFuliNRFLeH+DLDFzM4zswxwA7Br\n+g5mdo6FM22Z2dbwefvrXWzUrGnP8CefuYT/+x8+wtUXdPHnP3mVj/3Fo9y3u5dqtTFDVEUkGhYM\nd3cvA78P/ATYC/zQ3V8ys1vM7JZwt88CL5rZ88DtwA3eqAH0Tej8Qo6dN3Xzg+1Xkc9l+Y8/fJ7P\nfPMJ/t9+/egqImdGJzGtMNWqs+v5t/hv/+cV3jo2xtbNa7h12wVsu7CgaYhFRHPLNLuxiQrff/pX\nfOvnr/PWsTEuOqeDW7ddwKcuXUcqGZsLaInIDAr3iCiVq+x6/i12PLaffX1DbFrTxhc+ej6/eeUG\nWtI621UkbhTuEVOtOg/tPcR/f3Q/zx8YoNCR5d99+Dx+6wOb6GhJN7o8EVkmCveIcnf+YX8/dz22\nn8d/eZiOlhQ3ffBd3Pyh88jnNDmZSNQp3GNgT+8AOx7bz4MvvkMmmeDfvH8jn//18zWvvEiEKdxj\nZH9xiJ2Pvc59v+il6vCZ953Lb165gUs3rGZ1q1o2IlGicI+ht4+Ncvfjb/A3T/+KkVJwpuvmrjYu\n29DJZRtW876NnVxy7iraMrpGi0izUrjH2ODYBM8dGGBP7zGeD+/fOT4GQMJgy9oOLtuwmss2dnLZ\n+tVctK5DV40SaRIKdzlJ3/Ex9vQeY0/vAM+H90dHJgBIJ42L163i0vWruXT9ajauaePczlbWrW7R\ncEuRFUbhLvNyd3qPjk4F/p7eY7xw8BhD4+WT9svnsqzvbOHczlbWd7ZybnjbcFZwf1ZbWmfOiiwj\nXSBb5mVmbFzTxsY1bXzqsnVAMJa+9+goBweC21vh7eDAKK8eGuSRV/sYm6ie9Dyt6STnhuG/tqOF\nNe1pzmrPsKYtE9y3ZzirLbhf3ZrWFahElonCXaYkEsamrjY2dc0+lNLdOTJc4q2BsVk/AF4vDtM/\nPH7KB8AkM+hsnRH+bRk629Oc1ZbhrLY0q1uD+862DJ1taTrb0pH4PcDdmag4mZSmjpDloXCXmpkZ\nXbksXbksl25YPed+o6UKR0dKHBkunbgfLnFkZCK8D9YPHBlhT+8AR4ZLTFTmbg+2ppNB8LdNBn8Y\n/q1p2jJJ0slEcEslyCTtxHoyQSY1Yz2ZIJ0yMskE2XSSllSClnSSlnTyjL5VlCtV+odL9B0fpzg0\nRt/xcfoGxykOjtM3ODa1XBwcZ7xcpas9M/V7xsmtrhbWd7aSz2VJ6NuN1IHCXequNZOkNROEVi3c\nnZFShYHRCQZGSgyMTDAwMsHRkRLHwm1Hw20DIyVeOzQ0tVyu47z36aTRkkoGoZ8OQj87Ff4JWlLB\nh8DgeDkM7DH6h0vM9rNVZ1uatR1ZCh1Z3r95DWs7srRlUhwaHOOtgVHe7B/m7/cdZrhUOaWGdatb\np1pd6ztbWbe6lTXtGVa1pMi1pOhoSZPLpuhoSZFNJfSbh8xK4S4NZ2a0Z1O0Z1Osr/EDAYIPhXLV\nmahUmSg7pUo1WA5vpbKfWK5Umag4E+VgfbxcZbxcYWyiythEeF+uTC2PT1QYK1cYn9peZWBkgrGJ\nCrlsivWdLVy+sZNCR5a14a3QkWXtqhbyuUxNrSR35/hYeaq19daxsRPLA6M89foR3jk+RmWeD7B0\n0k4K++A+TUdLsN6eTYXfUoJvLdlUgkwqQTaVJJMKtgXrwf3k8mT9lapTcQ/uw1vVT9yXK8Hj1Srh\nfVBrNvxwbA1vk8stmeA19YG09BTu0rTMjHTYhiHT6GpOn5mxujXN6tY0F69bNes+5UqVvsFxBkYm\nGBybYHCszNB4OVgeLzM4FiwPjYXL42UODowG28bLDI2V6/rtph7MOCn0W9IJWjNJWlLJqfZaOmGk\nkkYqObmcIJ00UokEqWTQVkuF6+nprbhUguxU6y0ZPDa1LWzLhe26TDJJOnXiOVLJBKmEkUoYyYQ1\n/QeQwl1kBUslE1PDT89UtRp8qxkvVymF31hK5eDbTGlq27T7SvDNBSAZBl0yYSTNSIT307cnptYh\nmUjg7oyXq4xOVBgrBd+ARkvhengbnbZ9attEUNfIRIVypUq54kxUg/typcpE1U/dvoQfXOlk8P9K\nhx8oyakPgeADYeZvNNOHlZ9S1YwNN2zdyPaPXLA0hYdqCnczuwb4BpAEvu3ufzbjcQsfvw4YAf6t\nu++uc60icgYSCaMlkYzkCWmzteZKlSoT0z+8pq1PtuumbyuHLbtytUq56lMfJuXqieeuVIPRTuXJ\n5apTqVYxZhzd26yLwfq0bwJnr2pZujcltGC4m1kS+CbwSaAXeMbMdrn7y9N2uxbYEt4+ANwV3ouI\nLJlmb80tpVoG3W4F9rn76+5eAn4AXD9jn+uBez3wJNBpZuvqXKuIiNSolnBfDxyYtt4bbjvdfTCz\n7WbWY2Y9xWLxdGsVEZEaLevpcu6+09273b27UCgs50uLiMRKLeF+ENg4bX1DuO109xERkWVSS7g/\nA2wxs/PMLAPcAOyasc8u4CYLXAUcc/e361yriIjUaMHRMu5eNrPfB35CMBTyO+7+kpndEj6+A3iA\nYBjkPoKhkDcvXckiIrKQmsa5u/sDBAE+fduOacsO3Fbf0kRE5Exp/lERkQhq2JWYzKwI/OMZ/vM8\ncLiO5USR3qP56f1ZmN6j+TXq/XmXuy843LBh4b4YZtZTy2Wm4kzv0fz0/ixM79H8Vvr7o7aMiEgE\nKdxFRCKoWcN9Z6MLaAJ6j+an92dheo/mt6Lfn6bsuYuIyPya9chdRETm0XThbmbXmNmrZrbPzP6o\n0fWsRGb2ppm9YGbPmVlPo+tpNDP7jpn1mdmL07atMbOHzOyX4f1Zjayx0eZ4j/7EzA6Gf0fPmdl1\njayxkcxso5k9YmYvm9lLZvbFcPuK/TtqqnCfduGQa4H3ADea2XsaW9WK9TF3v3wlD9VaRvcA18zY\n9kfAw+6+BXg4XI+zezj1PQL4q/Dv6PLwTPW4KgNfdvf3AFcBt4XZs2L/jpoq3KntwiEiJ3H3nwNH\nZmy+HvhuuPxd4J8va1ErzBzvkYTc/e3JS4e6+yCwl+CaFSv276jZwr2mi4IIDvzUzJ41s+2NLmaF\nOnvazKXvAGc3spgV7N+b2Z6wbbNiWg6NZGabgSuAp1jBf0fNFu5Smw+7++UE7avbzOwjjS5oJQsn\nvtOwsVPdBZwPXA68DfxlY8tpPDPLAT8CvuTux6c/ttL+jpot3HVRkBq4+8Hwvg/4W4J2lpzs0OR1\nfsP7vgbXs+K4+yF3r7h7FfgWMf87MrM0QbB/z93vCzev2L+jZgv3Wi4cEmtm1m5mHZPLwG8AL87/\nr2JpF/A74fLvAH/XwFpWpBkXuf8XxPjvyMwMuBvY6+5fn/bQiv07arqTmMLhWH/NiQuH/NcGl7Si\nmNn5BEfrEMzX/zdxf4/M7PvANoJZ/A4BXwPuB34IbCKYnfRfu3tsf1Cc4z3aRtCSceBN4AtxvcKa\nmX0YeBx4AaiGm79C0HdfkX9HTRfuIiKysGZry4iISA0U7iIiEaRwFxGJIIW7iEgEKdxFRCJI4S4i\nEkEKdxGRCFK4i4hE0P8HUBlqWQ1rUgMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc5342c1d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
